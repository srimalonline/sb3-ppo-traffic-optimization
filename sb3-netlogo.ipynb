{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad3bd41-b851-4352-ab89-538b52d208a7",
   "metadata": {},
   "source": [
    "## 1. Impprt Dependacies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b68339ac-2a45-44b7-9743-eccd1a06c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete, MultiDiscrete, Dict\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9a910-8d6f-4223-826a-218fdca53682",
   "metadata": {},
   "source": [
    "## 3. Building an Environment\n",
    "- Build and agent to give us the best shower possible\n",
    "- Randomly temperature\n",
    "- 37 and 39 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8628f5ed-a2a5-48b7-a02f-0ce5ff5975cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_density_stdev():\n",
    "    return np.random.uniform(0, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0d6f7-3456-4c41-8e30-f715e71b9be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802901a6-c60e-4e0d-99e0-6274da765a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "338987b5-1a23-42e9-b1cd-4e7acad43a27",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2144672322.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[45], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.observation_space = Tuple((Discrete(7, start=1),Box(low=0, high=np.inf, shape=(8,), dtype=np.float32)))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "   self.action_space = Tuple((Discrete(7, start=1),Box(low=5, high=40, shape=(4,), dtype=int)))\n",
    "        self.observation_space = Tuple((Discrete(7, start=1),Box(low=0, high=np.inf, shape=(8,), dtype=np.float32)))\n",
    "        self.min_density = current_density_stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "46f767bb-ae6d-44e0-9c8c-a0601559f967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('com', 5), ('side-timers', array([38, 17, 23, 38]))])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dict({'com':Discrete(7, start=1), 'side-timers':Box(low=5, high=40, shape=(4,), dtype=int)}).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ac2f00e-5f55-41d2-a416-96a7054bc9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('com', 6),\n",
       "             ('side-timers',\n",
       "              array([0.18967883, 0.40962645, 1.8089591 , 1.1234088 , 0.316804  ,\n",
       "                     0.29596582, 0.56445277, 0.0802756 ], dtype=float32))])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dict({'com':Discrete(7, start=1), 'side-timers':Box(low=0, high=np.inf, shape=(8,), dtype=np.float32)}).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2dd9b1-0337-406a-816f-99ccd819ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSimulation(Env):\n",
    "    def __init__(self):\n",
    "        super(TrafficSimulation, self).__init__()\n",
    "        \n",
    "        # Define the observation space as a Dict\n",
    "        self.observation_space = Dict({\n",
    "            \"combination\": Discrete(7),  # Current route combination (com1 to com7)\n",
    "            \"densities\": Box(low=0, high=np.inf, shape=(8,), dtype=np.float32)  # Densities of S1 to S8\n",
    "        })\n",
    "        \n",
    "        # Define the action space as a MultiDiscrete\n",
    "        self.action_space = MultiDiscrete([\n",
    "            7,  # com: 7 possible combinations [1, 7]\n",
    "            36, 36, 36, 36  # side-timers: 4 values each ranging from 5 to 40 (5 + i where i is in range(0, 36))\n",
    "        ])\n",
    "        \n",
    "        # Initialize state\n",
    "        self.min_density = current_density_stdev()\n",
    "        self.state = self.min_density\n",
    "        self.max_steps = 100  # Limit to prevent infinite loops\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        \"\"\"\n",
    "        # Simulate new density with some conditions\n",
    "        new_density = current_density_stdev()\n",
    "\n",
    "        # Calculate reward\n",
    "        if self.min_density > new_density:\n",
    "            reward = 1\n",
    "            self.min_density = new_density\n",
    "        elif self.min_density == new_density:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # Check if the episode is done\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps  # Ensure the episode ends after max_steps\n",
    "        \n",
    "        # Optional additional info\n",
    "        info = {}\n",
    "\n",
    "        # Update state\n",
    "        self.state = new_density\n",
    "        \n",
    "        return np.array([self.state], dtype=np.float32), reward, done, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset the environment state.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "        \n",
    "        # Reset state to initial or random density\n",
    "        self.state = current_density_stdev()\n",
    "        self.min_density = self.state\n",
    "        self.current_step = 0  # Reset step count\n",
    "        \n",
    "        # Print statement can be uncommented for debugging\n",
    "        # print(\"Environment reset: Initial Density:\", self.state)\n",
    "        \n",
    "        return np.array([self.state], dtype=np.float32), {}\n",
    "        \n",
    "    def render(self):\n",
    "        # Implement visualization if needed\n",
    "        pass\n",
    "\n",
    "def current_density_stdev():\n",
    "    \"\"\"\n",
    "    Dummy function to return a simulated density value.\n",
    "    Modified to sometimes return lower densities.\n",
    "    \"\"\"\n",
    "    # Randomly decide if we want to generate a higher or lower density\n",
    "    if random.random() > 0.7:  # 30% chance to decrease density\n",
    "        return max(0, np.random.uniform(0, 5))  # Lower range for new densities\n",
    "    else:\n",
    "        return np.random.uniform(0, 10)  # Original range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c620b841-9ad8-4b1c-b25a-82ebac2f4c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mTrafficSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mTrafficSimulation.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(TrafficSimulation, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define the observation space as a Dict\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m \u001b[43mDict\u001b[49m({\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombination\u001b[39m\u001b[38;5;124m\"\u001b[39m: Discrete(\u001b[38;5;241m7\u001b[39m),  \u001b[38;5;66;03m# Current route combination (com1 to com7)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdensities\u001b[39m\u001b[38;5;124m\"\u001b[39m: Box(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Densities of S1 to S8\u001b[39;00m\n\u001b[1;32m      9\u001b[0m })\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Define the action space as a MultiDiscrete\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m MultiDiscrete([\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m7\u001b[39m,  \u001b[38;5;66;03m# com: 7 possible combinations [1, 7]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m36\u001b[39m, \u001b[38;5;241m36\u001b[39m, \u001b[38;5;241m36\u001b[39m, \u001b[38;5;241m36\u001b[39m  \u001b[38;5;66;03m# side-timers: 4 values each ranging from 5 to 40 (5 + i where i is in range(0, 36))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "env = TrafficSimulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9823748-f41f-4559-a4ee-185fe999039a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('combination', 6),\n",
       "             ('densities',\n",
       "              array([1.9184334 , 0.9233296 , 0.29225418, 0.9787818 , 0.06363887,\n",
       "                     0.82842344, 0.3184587 , 0.16957685], dtype=float32))])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40bdb41b-0a63-4f2d-a7b3-59644bba1a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 37,  8, 29])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8aa6ae-bdd8-4576-b98b-90254215518d",
   "metadata": {},
   "source": [
    "## 4. Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4cf80edb-8030-4641-8d2e-37b8d72c9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "412d619c-a02a-46a5-9547-5de7f5134ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.7581024], dtype=float32), -1, False, {})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7fe6f6fa-26ee-46a1-9715-0f57cb63cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: -96\n",
      "Episode: 2 Score: -90\n",
      "Episode: 3 Score: -92\n",
      "Episode: 4 Score: -86\n",
      "Episode: 5 Score: -88\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = TrafficSimulation()\n",
    "\n",
    "# Number of episodes to run\n",
    "episodes = 5\n",
    "for episode in range(1, episodes + 1):\n",
    "    # Reset the environment\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        \n",
    "        # Sample a random action from the action space\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action in the environment\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Accumulate the reward\n",
    "        score += reward\n",
    "\n",
    "    print('Episode: {} Score: {}'.format(episode, score))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76355581-3914-4a31-ad3c-89acdf3e100d",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04b85169-d1c6-41e8-9264-5c974405ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62cd2a53-2264-4f3c-b414-287ddf5f3ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Dict('com': Discrete(7, start=1), 'side-timers': Box(5, 40, (4,), int64)) was provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m log_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultiInputPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Workings/Jupyter/dsenv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:109\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     _init_setup_model: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    108\u001b[0m ):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgae_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgae_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43ment_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ment_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvf_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvf_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_buffer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_buffer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_buffer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_buffer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_init_setup_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiBinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# because of the advantage normalization\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[0;32m~/Desktop/Workings/Jupyter/dsenv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:85\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     62\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[38;5;241m.\u001b[39mSpace], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m ):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupport_multi_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupported_action_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m=\u001b[39m n_steps\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m gamma\n",
      "File \u001b[0;32m~/Desktop/Workings/Jupyter/dsenv/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:180\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vec_normalize_env \u001b[38;5;241m=\u001b[39m unwrap_vec_normalize(env)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m supported_action_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, supported_action_spaces), (\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe algorithm only supports \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_action_spaces\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as action spaces \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was provided\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m support_multi_env \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: the model does not support multiple envs; it requires \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma single vectorized environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Dict('com': Discrete(7, start=1), 'side-timers': Box(5, 40, (4,), int64)) was provided"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('Training' , 'Logs')\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0bd5728-d1a0-432c-bc7e-18da36043055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_4\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | -14.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 626      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -8.32        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 430          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025594546 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.417       |\n",
      "|    explained_variance   | 0.000871     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47.1         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000905    |\n",
      "|    value_loss           | 109          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -9.38       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 355         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003148612 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.378      |\n",
      "|    explained_variance   | 0.0219      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.7        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -9.04        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 329          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011861883 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.35        |\n",
      "|    explained_variance   | 0.0231       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 60.6         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | 0.000462     |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -11.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 314          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011393982 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | 0.0319       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 53.5         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    value_loss           | 121          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -8.68        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 312          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011657109 |\n",
      "|    clip_fraction        | 0.00845      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.413       |\n",
      "|    explained_variance   | 0.0174       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.3         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000201    |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -12          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 313          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011484574 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | -0.0067      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 65.5         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.000887    |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -12.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 310          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011635727 |\n",
      "|    clip_fraction        | 0.00503      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.375       |\n",
      "|    explained_variance   | 0.00909      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 51.7         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | 0.000321     |\n",
      "|    value_loss           | 93.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -15.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 305         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001557599 |\n",
      "|    clip_fraction        | 0.0314      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.339      |\n",
      "|    explained_variance   | 0.00168     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.9        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00209    |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -3.38       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 297         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006660042 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.00274     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 56.6        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.000348   |\n",
      "|    value_loss           | 123         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x148c281ecc0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f930e-cd6d-4e6a-a0e1-8b56bd9f4207",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c21432f3-27cc-468f-a62c-aea85bdea2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_path = os.path.join('Training', 'Saved Models', 'Shower_Model_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d990fe17-e0ac-4c19-8b2c-f8b7e7343a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(shower_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d7ef3-c5a1-45de-93de-1adde66cd91c",
   "metadata": {},
   "source": [
    "## Evaluvation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1278813c-0ea3-4481-95c0-fe084823a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e000d6c8-c6b7-4b3e-8b2e-b1aa5e8faa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(shower_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f97648c-38ca-4a0c-93f5-4b7ce40ec4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.0, 58.787753826796276)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be861b4-f850-4cb4-b472-91bfc5e354a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
